WARNING 02-06 15:55:10 [cuda.py:569] Detected different devices in the system: NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA A800 80GB PCIe, NVIDIA A800 80GB PCIe, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:14 [api_server.py:1272] vLLM API server version 0.14.1
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:14 [utils.py:263] non-default args: {'host': '0.0.0.0', 'port': 6767, 'model': 'Qwen/Qwen3-VL-Reranker-8B', 'trust_remote_code': True, 'dtype': 'bfloat16'}
[0;36m(APIServer pid=2974597)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2974597)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:36 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:36 [model.py:1545] Using max model len 262144
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:36 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:36 [vllm.py:630] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:55:36 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 02-06 15:55:54 [cuda.py:569] Detected different devices in the system: NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA A800 80GB PCIe, NVIDIA A800 80GB PCIe, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:55:58 [core.py:97] Initializing a V1 LLM engine (v0.14.1) with config: model='Qwen/Qwen3-VL-Reranker-8B', speculative_config=None, tokenizer='Qwen/Qwen3-VL-Reranker-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-VL-Reranker-8B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:56:08 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.200.14.82:59359 backend=nccl
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:56:08 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:57:40 [gpu_model_runner.py:3808] Starting to load model Qwen/Qwen3-VL-Reranker-8B...
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:57:40 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:57:41 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:57:52 [weight_utils.py:510] Time spent downloading weights for Qwen/Qwen3-VL-Reranker-8B: 0.645847 seconds
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.30s/it]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:03,  1.53s/it]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.27s/it]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.33s/it]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.34s/it]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m 
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:57:59 [default_loader.py:291] Loading weights took 5.60 seconds
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:00 [gpu_model_runner.py:3905] Model loading took 16.64 GiB memory and 18.575613 seconds
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:00 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 12288 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:40 [backends.py:644] Using cache directory: /home/pomegranar/.cache/vllm/torch_compile_cache/b04606fb3f/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:40 [backends.py:704] Dynamo bytecode transform time: 11.45 s
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:49 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 1.877 s
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:49 [monitor.py:34] torch.compile takes 13.33 s in total
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:50 [gpu_worker.py:358] Available KV cache memory: 52.73 GiB
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:50 [kv_cache_utils.py:1305] GPU KV cache size: 383,936 tokens
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:50 [kv_cache_utils.py:1310] Maximum concurrency for 262,144 tokens per request: 1.46x
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:03, 14.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:03, 14.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:03, 14.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:03, 14.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:00<00:02, 15.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:00<00:02, 15.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:00<00:02, 15.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:01<00:02, 15.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:01<00:02, 16.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:01<00:01, 16.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:01<00:01, 16.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:01<00:01, 17.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:01<00:01, 17.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:01<00:01, 16.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:01<00:01, 15.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:02<00:01, 16.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:02<00:01, 16.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:02<00:00, 16.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:02<00:00, 16.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:02<00:00, 16.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:02<00:00, 16.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:02<00:00, 16.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:02<00:00, 16.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:02<00:00, 16.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:03<00:00, 17.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:03<00:00, 16.32it/s]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   3%|â–Ž         | 1/35 [00:00<00:03,  9.49it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:02, 14.34it/s]Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 5/35 [00:00<00:01, 15.79it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:01, 16.50it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:01, 16.93it/s]Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:00<00:01, 17.09it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:00<00:01, 17.28it/s]Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:00<00:01, 17.41it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:01<00:01, 17.41it/s]Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:01<00:00, 17.37it/s]Capturing CUDA graphs (decode, FULL):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:01<00:00, 17.14it/s]Capturing CUDA graphs (decode, FULL):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:01<00:00, 16.96it/s]Capturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:01<00:00, 16.90it/s]Capturing CUDA graphs (decode, FULL):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:01<00:00, 17.14it/s]Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:01<00:00, 17.28it/s]Capturing CUDA graphs (decode, FULL):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 17.47it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:01<00:00, 17.62it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 17.75it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 17.04it/s]
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:56 [gpu_model_runner.py:4856] Graph capturing finished in 6 secs, took 0.64 GiB
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:58:56 [core.py:273] init engine (profile, create kv cache, warmup model) took 56.85 seconds
[0;36m(EngineCore_DP0 pid=2977339)[0;0m INFO 02-06 15:59:08 [vllm.py:630] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:08 [api_server.py:1014] Supported tasks: ['generate']
[0;36m(APIServer pid=2974597)[0;0m WARNING 02-06 15:59:09 [model.py:1358] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:09 [serving_responses.py:224] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:10 [serving_chat.py:146] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:10 [serving_chat.py:182] Warming up chat template processing...
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:23 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:24 [serving_chat.py:218] Chat template warmup completed in 13786.3ms
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:25 [serving_completion.py:78] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [serving_chat.py:146] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [api_server.py:1346] Starting vLLM API server 0 on http://0.0.0.0:6767
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO 02-06 15:59:26 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=2974597)[0;0m INFO:     Started server process [2974597]
[0;36m(APIServer pid=2974597)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=2974597)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=2974597)[0;0m WARNING 02-07 05:41:44 [api_router.py:128] To indicate that the rerank API is not part of the standard OpenAI API, we have located it at `/rerank`. Please update your client accordingly. (Note: Conforms to JinaAI rerank API)
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:59426 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:52028 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:60540 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:41202 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:40024 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:57146 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:49600 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:47920 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:44290 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:45202 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:60816 - "POST /v1/rerank/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:59484 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:56494 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:51050 - "POST /v1 HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:47064 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:38826 - "POST /v1/rerank/chat/completions HTTP/1.1" 404 Not Found
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:53238 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:54002 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:44822 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:44374 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:56250 - "POST /v1/rerank HTTP/1.1" 200 OK
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:45526 - "GET /v1/rerank HTTP/1.1" 405 Method Not Allowed
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:36408 - "GET /v1/rerank HTTP/1.1" 405 Method Not Allowed
[0;36m(APIServer pid=2974597)[0;0m INFO:     127.0.0.1:36418 - "GET /v1/rerank HTTP/1.1" 405 Method Not Allowed
